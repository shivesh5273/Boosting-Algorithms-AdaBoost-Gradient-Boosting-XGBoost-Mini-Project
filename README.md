# Boosting-Algorithms-AdaBoost-Gradient-Boosting-XGBoost-Mini-Project

This project demonstrates three major boosting approaches using the Wine dataset:

- **AdaBoost** (adaptive boosting with decision stumps)
- **Gradient Boosting** (stage-wise additive modeling with trees)
- **XGBoost** (optimized, regularized gradient boosted trees)

## Whatâ€™s inside
- Clean train/test evaluation
- Accuracy, Balanced Accuracy, Macro F1
- Confusion matrices + classification reports
- 2D decision boundary plots (binary subset)
- Multiclass comparison on full dataset
- Feature importance visualizations

